{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as iter_t\n",
    "\n",
    "retail_data = [i.strip().split() for i in open(\"retail.dat\").readlines()]\n",
    "\n",
    "retail_data = pd.DataFrame(retail_data)\n",
    "\n",
    "# allocate a limited number of rows, for testing\n",
    "retail_data = retail_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(data, min_support=0.05, min_confidence=0.10):\n",
    "    \n",
    "    total_transactions = len(data)\n",
    "    \n",
    "    reduction = 5\n",
    "    \n",
    "    data = data.sample(n=int(total_transactions/reduction))\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    min_support = min_support / reduction\n",
    "    \n",
    "    def n_candidate(prev_L, n):\n",
    "        names_helper = np.array([], dtype=int)\n",
    "        for i in range(n-1):\n",
    "            names_helper = np.append(names_helper, prev_L[i].to_numpy(dtype=int))\n",
    "            \n",
    "        candidate         = iter_t.combinations(set(names_helper), n)\n",
    "        candidate         = pd.DataFrame([i for i in candidate])\n",
    "        if len(candidate) == 0:\n",
    "            return candidate\n",
    "        candidate[\"freq\"] = np.zeros(len(candidate[0]), dtype=int)\n",
    "        \n",
    "        candidate.is_copy = False\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(candidate.shape[0]):\n",
    "                if set(candidate.loc[j][0:n].to_numpy(dtype=int)).issubset(set(data.loc[i].dropna().to_numpy(dtype=int))):\n",
    "                    candidate.loc[j, \"freq\"] += 1\n",
    "        return candidate\n",
    "    \n",
    "    # get unique items\n",
    "    # construct array of all items\n",
    "    items     = []\n",
    "    hash_dict = {}\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[0]):\n",
    "            items.append(data[i][j])\n",
    "        \n",
    "    unique_items = list(set(items))\n",
    "    \n",
    "    # construct candidate sets\n",
    "    C = []\n",
    "    L = []\n",
    "    \n",
    "    C1 = []\n",
    "    for val in unique_items:\n",
    "        C1.append((val, items.count(val)))\n",
    "    \n",
    "    #total_transactions = len(data)\n",
    "    \n",
    "    del(items)\n",
    "    \n",
    "    C1 = pd.DataFrame(C1, columns=[0, \"freq\"], dtype=int)\n",
    "    C1 = C1.dropna()\n",
    "    \n",
    "    C1[\"conf\"] = np.ones(len(C1[0]), dtype=float)\n",
    "    C1[\"sup\"]  = C1[\"freq\"] / total_transactions\n",
    "    \n",
    "    L1 = C1[C1[\"sup\"] >= min_support]\n",
    "    L1 = L1.astype({0: int})\n",
    "    \n",
    "    C.append(C1)\n",
    "    L.append(L1)\n",
    "    \n",
    "    def conf(_L, prevL, n):\n",
    "        # Build confidence\n",
    "        pd.set_option('mode.chained_assignment', 'warn')\n",
    "        _L[\"conf\"] = np.zeros(len(_L[0]), dtype=float)\n",
    "\n",
    "        _L.is_copy = False\n",
    "        for i in range(len(_L[\"freq\"])):\n",
    "            oldSup = _L[\"freq\"].iloc[i]                                        #(prevL[\"freq\"][prevL.iloc[:, 0:n] == L_cur[0:n]]).iat[0]\n",
    "            for j in range(len(prevL[0])):\n",
    "                if set(prevL.iloc[j, 0:n-1]) == set(_L.iloc[i, 0:n-1]):\n",
    "                    oldSup = prevL[\"freq\"].iloc[j]\n",
    "                    break\n",
    "            _L[\"conf\"].iloc[i] = (_L[\"freq\"].iloc[i] / oldSup)\n",
    "        \n",
    "        return _L\n",
    "        \n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    if len(L1[0]) == 0:\n",
    "        return L\n",
    "    \n",
    "    while True:\n",
    "        cand         = n_candidate(L[i-2], i)\n",
    "        if len(cand) == 0:\n",
    "            break\n",
    "        cand[\"sup\"]  = cand[\"freq\"] / total_transactions\n",
    "        L_cur        = cand[cand[\"sup\"] >= min_support]\n",
    "        \n",
    "        if len(L_cur[0]) == 0:\n",
    "            break\n",
    "        \n",
    "        L_cur = conf(L_cur, L[i-2], i)\n",
    "        \n",
    "        L_cur = L_cur[L_cur[\"conf\"] >= min_confidence]\n",
    "        \n",
    "        C.append(cand)\n",
    "        L.append(L_cur)\n",
    "        i += 1\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = random_sampling(retail_data, min_support=0.15)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(data, min_support=0.15, min_confidence=0.10):\n",
    "    def n_candidate(prev_L, n):\n",
    "        names_helper = np.array([], dtype=int)\n",
    "        for i in range(n-1):\n",
    "            names_helper = np.append(names_helper, prev_L[i].to_numpy(dtype=int))\n",
    "\n",
    "        candidate         = iter_t.combinations(set(names_helper), n)\n",
    "        candidate         = pd.DataFrame([i for i in candidate])\n",
    "        candidate[\"freq\"] = np.zeros(len(candidate[0]), dtype=int)\n",
    "\n",
    "        candidate.is_copy = False\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(candidate.shape[0]):\n",
    "                if set(candidate.loc[j][0:n].to_numpy(dtype=int)).issubset(set(data.loc[i].dropna().to_numpy(dtype=int))):\n",
    "                    candidate.loc[j, \"freq\"] += 1\n",
    "        return candidate\n",
    "    \n",
    "    # get unique items\n",
    "    # construct array of all items\n",
    "    items = []\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[0]):\n",
    "            items.append(data[i][j])\n",
    "    unique_items = list(set(items))\n",
    "    \n",
    "    # construct candidate sets\n",
    "    C = []\n",
    "    L = []\n",
    "    \n",
    "    C1 = []\n",
    "    for val in unique_items:\n",
    "        C1.append((val, items.count(val)))\n",
    "    \n",
    "    del(items)\n",
    "    \n",
    "    C1 = pd.DataFrame(C1, columns=[0, \"freq\"], dtype=int)\n",
    "    C1 = C1.dropna()\n",
    "    \n",
    "    C1[\"conf\"] = np.ones(len(C1[0]), dtype=float)\n",
    "    \n",
    "    L1 = C1[C1[\"freq\"] >= min_support]\n",
    "    L1 = L1.astype({0: int})\n",
    "    \n",
    "    C.append(C1)\n",
    "    L.append(L1)\n",
    "    \n",
    "    def conf(_L, prevL, n):\n",
    "        # Build confidence\n",
    "        pd.set_option('mode.chained_assignment', 'warn')\n",
    "        _L[\"conf\"] = np.zeros(len(_L[0]), dtype=float)\n",
    "\n",
    "        _L.is_copy = False\n",
    "        for i in range(len(_L[\"freq\"])):\n",
    "            oldSup = _L[\"freq\"].iloc[i]                                        #(prevL[\"freq\"][prevL.iloc[:, 0:n] == L_cur[0:n]]).iat[0]\n",
    "            for j in range(len(prevL[0])):\n",
    "                if set(prevL.iloc[j, 0:n-1]) == set(_L.iloc[i, 0:n-1]):\n",
    "                    oldSup = prevL[\"freq\"].iloc[j]\n",
    "                    break\n",
    "            _L[\"conf\"].iloc[i] = (_L[\"freq\"].iloc[i] / oldSup)\n",
    "        \n",
    "        return _L\n",
    "        \n",
    "    \n",
    "    i = 2\n",
    "    while True:\n",
    "        cand  = n_candidate(L[i-2], i)\n",
    "        L_cur = cand[cand[\"freq\"] >= min_support]\n",
    "        \n",
    "        if len(L_cur[0]) == 0:\n",
    "            break\n",
    "        \n",
    "        L_cur = conf(L_cur, L[i-2], i)\n",
    "        \n",
    "        L_cur = L_cur[L_cur[\"conf\"] >= min_confidence]\n",
    "        \n",
    "        C.append(cand)\n",
    "        L.append(L_cur)\n",
    "        i += 1\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "def apriori_SON(data, chunks=2, min_support=0.15, min_confidence=0.10):\n",
    "    \n",
    "    #calcuclate local support amount based on s*n/k formula\n",
    "    chunk_size=data.shape[0]/chunks\n",
    "    ls = min_support*chunks\n",
    "    \n",
    "    #PASS 1\n",
    "    #split dataset into chunks\n",
    "    chunk_list = np.array_split(data, chunks)\n",
    "\n",
    "    #apply apriori to each chunk to get the local candidate sets (stored in results)\n",
    "    results = []\n",
    "    for chunk in chunk_list:\n",
    "        results.append(apriori(chunk, min_support=ls, min_confidence=min_confidence))\n",
    "    \n",
    "    #PASS 2\n",
    "    #sort each candidate set\n",
    "    for i in range(0, chunks):\n",
    "        for candidate in results[i]:\n",
    "            candidate = sorted(candidate)\n",
    "            \n",
    "    #create dictionary for counts\n",
    "    d = {}\n",
    "    for i in range(0, chunks):\n",
    "        for candidate in results[i]:\n",
    "            if candidate not in d:\n",
    "                d[candidate] = 0\n",
    "    \n",
    "    #for each chunk, go through and count how many times each candidate set appears\n",
    "    for i in range(0,chunks):\n",
    "        #for each candidate set from that chunk\n",
    "        for candidate in results[i]:\n",
    "            #go through each bucket of that chunk and \n",
    "            for bucket in chunk_list[i]:\n",
    "                #check if the candidate set exists within it, if so count it\n",
    "                if all(item in candidate for item in bucket):\n",
    "                    d[candidate]+=1\n",
    "                    \n",
    "    #remove those with support lower than threshold             \n",
    "    return dict((key,value) for key, value in d.iteritems() if value >=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = apriori_SON(retail_data, chunks=2, min_support=0.15)\n",
    "#res = apriori(retail_data, min_support=0.15)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
