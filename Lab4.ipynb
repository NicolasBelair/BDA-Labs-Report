{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as iter_t\n",
    "import copy as copy\n",
    "\n",
    "retail_data = [i.strip().split() for i in open(\"retail.dat\").readlines()]\n",
    "\n",
    "retail_data = pd.DataFrame(retail_data)\n",
    "\n",
    "# allocate a limited number of rows, for testing\n",
    "retail_data = retail_data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(data, min_support=0.05, min_confidence=0.10):\n",
    "    \n",
    "    total_transactions = len(data)\n",
    "    \n",
    "    reduction = 5\n",
    "    \n",
    "    data = data.sample(n=int(total_transactions/reduction))\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "    min_support = min_support / reduction\n",
    "    \n",
    "    def n_candidate(prev_L, n):\n",
    "        names_helper = np.array([], dtype=int)\n",
    "        for i in range(n-1):\n",
    "            names_helper = np.append(names_helper, prev_L[i].to_numpy(dtype=int))\n",
    "            \n",
    "        candidate         = iter_t.combinations(set(names_helper), n)\n",
    "        candidate         = pd.DataFrame([i for i in candidate])\n",
    "        if len(candidate) == 0:\n",
    "            return candidate\n",
    "        candidate[\"freq\"] = np.zeros(len(candidate[0]), dtype=int)\n",
    "        \n",
    "        candidate.is_copy = False\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(candidate.shape[0]):\n",
    "                if set(candidate.loc[j][0:n].to_numpy(dtype=int)).issubset(set(data.loc[i].dropna().to_numpy(dtype=int))):\n",
    "                    candidate.loc[j, \"freq\"] += 1\n",
    "        return candidate\n",
    "    \n",
    "    # get unique items\n",
    "    # construct array of all items\n",
    "    items     = []\n",
    "    hash_dict = {}\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[0]):\n",
    "            items.append(data[i][j])\n",
    "        \n",
    "    unique_items = list(set(items))\n",
    "    \n",
    "    # construct candidate sets\n",
    "    C = []\n",
    "    L = []\n",
    "    \n",
    "    C1 = []\n",
    "    for val in unique_items:\n",
    "        C1.append((val, items.count(val)))\n",
    "    \n",
    "    #total_transactions = len(data)\n",
    "    \n",
    "    del(items)\n",
    "    \n",
    "    C1 = pd.DataFrame(C1, columns=[0, \"freq\"], dtype=int)\n",
    "    C1 = C1.dropna()\n",
    "    \n",
    "    C1[\"conf\"] = np.ones(len(C1[0]), dtype=float)\n",
    "    C1[\"sup\"]  = C1[\"freq\"] / total_transactions\n",
    "    \n",
    "    L1 = C1[C1[\"sup\"] >= min_support]\n",
    "    L1 = L1.astype({0: int})\n",
    "    \n",
    "    C.append(C1)\n",
    "    L.append(L1)\n",
    "    \n",
    "    def conf(_L, prevL, n):\n",
    "        # Build confidence\n",
    "        pd.set_option('mode.chained_assignment', 'warn')\n",
    "        _L[\"conf\"] = np.zeros(len(_L[0]), dtype=float)\n",
    "\n",
    "        _L.is_copy = False\n",
    "        for i in range(len(_L[\"freq\"])):\n",
    "            oldSup = _L[\"freq\"].iloc[i]                                        #(prevL[\"freq\"][prevL.iloc[:, 0:n] == L_cur[0:n]]).iat[0]\n",
    "            for j in range(len(prevL[0])):\n",
    "                if set(prevL.iloc[j, 0:n-1]) == set(_L.iloc[i, 0:n-1]):\n",
    "                    oldSup = prevL[\"freq\"].iloc[j]\n",
    "                    break\n",
    "            _L[\"conf\"].iloc[i] = (_L[\"freq\"].iloc[i] / oldSup)\n",
    "        \n",
    "        return _L\n",
    "        \n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    if len(L1[0]) == 0:\n",
    "        return L\n",
    "    \n",
    "    while True:\n",
    "        cand         = n_candidate(L[i-2], i)\n",
    "        if len(cand) == 0:\n",
    "            break\n",
    "        cand[\"sup\"]  = cand[\"freq\"] / total_transactions\n",
    "        L_cur        = cand[cand[\"sup\"] >= min_support]\n",
    "        \n",
    "        if len(L_cur[0]) == 0:\n",
    "            break\n",
    "        \n",
    "        L_cur = conf(L_cur, L[i-2], i)\n",
    "        \n",
    "        L_cur = L_cur[L_cur[\"conf\"] >= min_confidence]\n",
    "        \n",
    "        C.append(cand)\n",
    "        L.append(L_cur)\n",
    "        i += 1\n",
    "    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oboutte/.local/lib/python2.7/site-packages/ipykernel_launcher.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[      0  freq  conf   sup\n",
       " 75   39    10   1.0  0.10\n",
       " 76   38     4   1.0  0.04\n",
       " 115  48     7   1.0  0.07\n",
       " 117  41     5   1.0  0.05,     0   1  freq   sup  conf\n",
       " 5  38  39     4  0.04   1.0]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = random_sampling(retail_data, min_support=0.15)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(data, min_support=0.05, min_confidence=0.10):\n",
    "    def n_candidate(prev_L, n):\n",
    "        names_helper = np.array([], dtype=int)\n",
    "        for i in range(n-1):\n",
    "            names_helper = np.append(names_helper, prev_L[i].to_numpy(dtype=int))\n",
    "\n",
    "        candidate         = iter_t.combinations(set(names_helper), n)\n",
    "        candidate         = pd.DataFrame([i for i in candidate])\n",
    "        candidate[\"freq\"] = np.zeros(len(candidate[0]), dtype=int)\n",
    "\n",
    "        candidate.is_copy = False\n",
    "\n",
    "        for i in range(data.shape[0]):\n",
    "            for j in range(candidate.shape[0]):\n",
    "                if set(candidate.loc[j][0:n].to_numpy(dtype=int)).issubset(set(data.loc[i].dropna().to_numpy(dtype=int))):\n",
    "                    candidate.loc[j, \"freq\"] += 1\n",
    "        return candidate\n",
    "    \n",
    "    # get unique items\n",
    "    # construct array of all items\n",
    "    items = []\n",
    "    for i in range(data.shape[1]):\n",
    "        for j in range(data.shape[0]):\n",
    "            items.append(data[i][j])\n",
    "    unique_items = list(set(items))\n",
    "    \n",
    "    # construct candidate sets\n",
    "    C = []\n",
    "    L = []\n",
    "    \n",
    "    C1 = []\n",
    "    for val in unique_items:\n",
    "        C1.append((val, items.count(val)))\n",
    "    \n",
    "    del(items)\n",
    "    \n",
    "    C1 = pd.DataFrame(C1, columns=[0, \"freq\"], dtype=int)\n",
    "    C1 = C1.dropna()\n",
    "    \n",
    "    C1[\"conf\"] = np.ones(len(C1[0]), dtype=float)\n",
    "    \n",
    "    L1 = C1[C1[\"freq\"] >= min_support]\n",
    "    L1 = L1.astype({0: int})\n",
    "    \n",
    "    C.append(C1)\n",
    "    L.append(L1)\n",
    "    \n",
    "    def conf(_L, prevL, n):\n",
    "        # Build confidence\n",
    "        pd.set_option('mode.chained_assignment', 'warn')\n",
    "        _L[\"conf\"] = np.zeros(len(_L[0]), dtype=float)\n",
    "\n",
    "        _L.is_copy = False\n",
    "        for i in range(len(_L[\"freq\"])):\n",
    "            oldSup = _L[\"freq\"].iloc[i]                                        #(prevL[\"freq\"][prevL.iloc[:, 0:n] == L_cur[0:n]]).iat[0]\n",
    "            for j in range(len(prevL[0])):\n",
    "                if set(prevL.iloc[j, 0:n-1]) == set(_L.iloc[i, 0:n-1]):\n",
    "                    oldSup = prevL[\"freq\"].iloc[j]\n",
    "                    break\n",
    "            _L[\"conf\"].iloc[i] = (_L[\"freq\"].iloc[i] / oldSup)\n",
    "        \n",
    "        return _L\n",
    "        \n",
    "    \n",
    "    i = 2\n",
    "    while True:\n",
    "        cand  = n_candidate(L[i-2], i)\n",
    "        L_cur = cand[cand[\"freq\"] >= min_support]\n",
    "        \n",
    "        if len(L_cur[0]) == 0:\n",
    "            break\n",
    "        \n",
    "        L_cur = conf(L_cur, L[i-2], i)\n",
    "        \n",
    "        L_cur = L_cur[L_cur[\"conf\"] >= min_confidence]\n",
    "        \n",
    "        C.append(cand)\n",
    "        L.append(L_cur)\n",
    "        i += 1\n",
    "    \n",
    "    return L\n",
    "\n",
    "\n",
    "\n",
    "def apriori_SON(data, chunks=2, min_support=0.15, min_confidence=0.10):\n",
    "    \n",
    "    #calcuclate local support amount based on s*n/k formula\n",
    "    chunk_size=data.shape[0]/chunks\n",
    "    ls = min_support*chunks\n",
    "    \n",
    "    #PASS 1\n",
    "    #split dataset into chunks\n",
    "    chunk_list = np.array_split(data, chunks)\n",
    "\n",
    "    #apply apriori to each chunk to get the local candidate sets (stored in results)\n",
    "    results = []\n",
    "    for chunk in chunk_list:\n",
    "        chunk.reset_index(inplace=True, drop=True)\n",
    "        results.append(apriori(chunk, min_support=ls, min_confidence=min_confidence))\n",
    "    \n",
    "    #PASS 2\n",
    "    #sort each candidate set\n",
    "    for i in range(0, chunks):\n",
    "        for candidate in results[i]:\n",
    "            col_list = []\n",
    "            for x in range(candidate.shape[1]-2):\n",
    "                col_list.append(x)\n",
    "            candidate = candidate.sort_values(by=col_list, ascending=True)\n",
    "            \n",
    "    #create dictionary for counts\n",
    "    d = {}\n",
    "    for i in range(0, chunks):\n",
    "        for candidate in results[i]:\n",
    "            col_list = []\n",
    "            for x in range(candidate.shape[1]-2):\n",
    "                col_list.append(x)\n",
    "            for cand in (candidate.loc[:,col_list]).to_numpy():\n",
    "                if tuple(cand) not in d:\n",
    "                    d[tuple(cand)] = 0\n",
    "    \n",
    "    #for each chunk, go through and count how many times each candidate set appears \n",
    "    for chunk in results:\n",
    "        for cand in chunk:\n",
    "            n = cand.shape[1]-2\n",
    "            cand.reset_index(inplace=True, drop=True) # need to be able to properly index\n",
    "            for i in range(data.shape[0]):\n",
    "                    for j in range(cand.shape[0]):\n",
    "                        if set(cand.loc[j][0:n].to_numpy(dtype=int)).issubset(set(data.loc[i].dropna().to_numpy(dtype=int))):\n",
    "                            d[tuple(cand.loc[j][0:n].to_numpy())] += 1\n",
    "                    \n",
    "    #remove those with support lower than threshold   \n",
    "    return dict([(key,value) for key, value in d.items() if (value/data.shape[0]) >= min_support])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oboutte/.local/lib/python2.7/site-packages/pandas/core/generic.py:5079: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  object.__getattribute__(self, name)\n",
      "/home/oboutte/.local/lib/python2.7/site-packages/pandas/core/generic.py:5080: FutureWarning: Attribute 'is_copy' is deprecated and will be removed in a future version.\n",
      "  return object.__setattr__(self, name, value)\n"
     ]
    }
   ],
   "source": [
    "res = apriori_SON(retail_data, chunks=2, min_support=0.40)\n",
    "#res = apriori(retail_data, min_support=0.15)\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
